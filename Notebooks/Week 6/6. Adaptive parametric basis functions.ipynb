{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science for Mobility / Intro to Business Analytics\n",
    "\n",
    "# Lecture 6 - Adaptive parametric basis functions\n",
    "\n",
    "In this notebook, we will see how adaptive parametric basis functions can be used to achieve complex decision boundaries for classification problems.\n",
    "\n",
    "We begin by importing the packages that we will need later on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# fix random generator seed (for reproducibility of results)\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#matplotlib style options\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't noticed yet, for some strange reason, it seems that we often need to run the commands for setting the matplotlib style options twice in order for them to take effect. So let's run them again, just to make sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matplotlib style options\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the demo in this notebook, we will use some data that we previously prepared. It corresponds to a binary classification problem with just 2 features: X1 and X2. Let's load the data with Pandas and visualize it using Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load demo dataset from file\n",
    "df = pd.read_csv(\"demo_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features and targets as seperate numpy matrices/arrays\n",
    "X = df[[\"X1\",\"X2\"]].to_numpy()\n",
    "y_true = df[\"class\"].to_numpy()\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize dataset and color points according to class\n",
    "plt.scatter(X[y_true==0,0], X[y_true==0,1], marker='x', color=\"r\")\n",
    "plt.scatter(X[y_true==1,0], X[y_true==1,1], marker='o', color=\"b\")\n",
    "plt.legend([\"class 0\", \"class 1\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imediately we can observe that the classes are not linearly separable! Indeed, any potential decision boundary that can separate the two classes would have to be quite complex.\n",
    "\n",
    "But let's try to fit a simple logistic regression model in Sklearn anyway to see what decision boundaries it can achieve on this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression model\n",
    "\n",
    "Recall the logistic regression model is a linear model on the features X of the form:\n",
    "\n",
    "\\begin{align}\n",
    "y = p(y_j=1)=\\mbox{sigmoid}(\\pmb{\\beta}^T{\\textbf{x}}_{\\boldsymbol{j}})=\\frac{\\displaystyle 1}{\\displaystyle 1+\\exp(-\\pmb{\\beta}^T{\\textbf{x}}_{\\boldsymbol{j}})}\n",
    "\\end{align}\n",
    "\n",
    "Two important notes:\n",
    "\n",
    "1) Since we want to be able to visualize and study the decision boundaries, we will restrict all models to NOT FIT an intercept/bias term. Otherwise, the model will have 3 instead of 2 tunable parameters, and visualizing decision boundaries in 3D becomes way more complicated. In Sklearn, this can be done by passing the option \"fit_intercept=False\" to the LogisticRegression model.\n",
    "\n",
    "2) In this case, we don't care about overfitting. In fact, we want our models to overfit a bit in order to study how complex the decision boundaries can really get. In Sklearn, this can be done by increasing the value of the \"C\" parameter of the LogisticRegression model. For example, C=10000.\n",
    "\n",
    "Can you do it? Make sure to call your logistic regression model \"reg\", so that the rest of the code works!! :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the accuracy (on the trainset)? Can you calculate it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very good, right? Is it surprising given how the dataset looks like and what the classes are?\n",
    "\n",
    "Now, let's have a look at the decision boundary that the logistic regression model that you fitted induces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid\n",
    "xx1, xx2 = np.mgrid[-3.5:2.5:.01, -2.2:4:.01]\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()]   #if you want to know what this is, inspect the \"grid\" variable,\n",
    "\n",
    "# compute probabilities for all points in the grid\n",
    "probs = reg.predict_proba(grid)[:,1]\n",
    "\n",
    "# make plot\n",
    "contour = plt.contourf(xx1, xx2, probs.reshape(xx1.shape), 25, cmap=\"RdBu\", vmin=0, vmax=1)\n",
    "plt.scatter(X[:,0], X[:, 1], c=y_true, s=100, cmap=\"RdBu\", vmin=-.2, vmax=1.2, edgecolor=\"white\", linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, the colors of the points indicate which class they truly belong to, and the shades/gradient colors in the background illustrate the probability of points in that area belonging to a given class. For example, the darker blue the color is, the more likely that area of the feature space is to belong to the blue class (according to the logistic regression model!). The decision boundary is then located in the line where red turns to blue. \n",
    "\n",
    "Well, given the assumption of the logistic regression model about the linearity of the decision boundary, the learned decision boundary makes sense, right? Can you come up with a linear decision boundary that does a better job than this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis functions\n",
    "\n",
    "Ok, but in the end this is not a very good classifier. How can we do a better job at classifying? \n",
    "\n",
    "Basis functions! We can obtain arbitrarily complex decision boundaries if use the \"right\" basis functions $\\phi(\\textbf{x})$. The model can still be linear, but the basis transformations allow the decision boundary to be non-linear. \n",
    "\n",
    "\\begin{align}\n",
    "y=\\color{blue}{ \\mbox{sigmoid}\\Bigg(b + \\sum_{m=1}^M w_m } \\, \\color{red}{ {\\phi}_m (\\textbf{x}) } \\color{blue}{ \\Bigg) }\n",
    "\\end{align}\n",
    "\n",
    "But how can come up with \"good\" basis functions $\\phi(\\textbf{x})$? There are different approaches:\n",
    "\n",
    "- **Manual feature engineering.** If you have good domain knowledge that you can exploit to come up with good features that make it easier for the model to discriminate between classes, then this is a good option. The problem is that it can be very time-consuming, and the number of extra features may end up being too high - leads to the curse of dimensionality...\n",
    "\n",
    "- **Kernel trick.** You can augment the feature space using for example a Gaussian RBF kernel, as typically done in SVMs. But where to place the centers of the inducing points? And how many? By default, we commonly assume that each data point in X is also a center. But then we have as many basis functions (and parameters to tune!) as training points! Overfitting is bound to be severe. This is where SVMs do something very clever, and only keep a subset of inducing points that are helpful for maximizing the margin between classes - called the \"support vectors\". But the size of this subset still typically grows with the size of the dataset... :-(\n",
    "\n",
    "- **Adaptive parametric basis functions.** We fix the number of basis functions in advance! BUT, we allowed them to be adaptive. How? Define them using a (non-linear) parametric form, in which the parameters of the basis functions are adapted during training. We will discuss later how to train them, but this is what neural networks do! \n",
    "\n",
    "\\begin{align}\n",
    "y=\\color{blue}{ \\mbox{sigmoid}\\Bigg( b + \\sum_{m=1}^M w_m} \\, \\color{red}{ g\\bigg(b_{m} + \\sum_{i=1}^D w_{m,i} x_i \\bigg) } \\color{blue}{ \\Bigg) }\n",
    "\\end{align}\n",
    "\n",
    "We can write the part in red more compactly using matrix notation: \n",
    "\n",
    "\\begin{align}\n",
    "\\color{red}{ \\textbf{h} = g(\\textbf{b} + \\textbf{x} \\textbf{W}) }\n",
    "\\end{align}\n",
    "\n",
    "We can also think of this visually by representing the operations in graph (network):\n",
    "\n",
    "\n",
    "<div>\n",
    "    <br/>\n",
    "<img src=\"http://mlsm.man.dtu.dk/wp-content/uploads/2019/10/nn_basis-1.png\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, the goal is to learn the parameters of basis transformation in red. However, for the purpose of this notebook tutorial, we will consider them to be given in advance. Just so that we can see their potential...\n",
    "\n",
    "Here are the values of $\\textbf{W}$ and $\\textbf{b}$ given in advance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the basis functions\n",
    "W = np.array([[-1.53411417,  1.27767682,  0.33231401, -0.74848654],\n",
    "              [ 1.55115198,  0.11567463,  1.17929718,  0.06751848]])\n",
    "print(\"W shape:\", W.shape)\n",
    "b = np.array([-1.59442766, -0.59937502,  0.0052437 ,  0.04698059])\n",
    "print(\"b shape:\", b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the dimensions of $\\textbf{W}$ and $\\textbf{b}$. We have a-priori fixed the number of basis functions to be 4. Of course, we could have used a different number...\n",
    "\n",
    "Now, we use these parameters to compute the transformed features $\\textbf{h}$. You can think of $\\textbf{h}$ as a re-representation (non-linear tranformation) of the original features $\\textbf{X}$ in a higher-dimensional space. In this case, the original space of $\\textbf{X}$ was 2-dimensional, while the transformed space is 4-dimensional.\n",
    "\n",
    "Can you calculate $\\textbf{h}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dimensions/shape of $\\textbf{h}$. Does it make sense?\n",
    "\n",
    "You can also have a look at the values of $\\textbf{h}$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we have a transformed version of $\\textbf{X}$. Let's try to fit a simple logistic regression classifier to this new representation of the data (use the same value of \"C\" as before, and also use \"fit_intercept=False\"). Make sure to call your logistic regression model variable \"reg\" for the rest of the code to work :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the accuracy (on the trainset)? Can you calculate it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better, right? \n",
    "\n",
    "Now let's have a look at the decision boundary that the new logistic regression classifier based on the new transformed features induces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid\n",
    "xx1, xx2 = np.mgrid[-3.5:2.5:.01, -2.2:4:.01]\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()]   #if you want to know what this is, inspect the \"grid\" variable,\n",
    "\n",
    "# compute probabilities for all points in the grid\n",
    "h_grid = sigmoid(b + np.dot(grid,W))\n",
    "probs = reg.predict_proba(h_grid)[:,1]\n",
    "\n",
    "# make plot\n",
    "contour = plt.contourf(xx1, xx2, probs.reshape(xx1.shape), 25, cmap=\"RdBu\", vmin=0, vmax=1)\n",
    "plt.scatter(X[:,0], X[:, 1], c=y_true, s=100, cmap=\"RdBu\", vmin=-.2, vmax=1.2, edgecolor=\"white\", linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, right? Quite a complex decision boundary!\n",
    "\n",
    "Try playing with the values of \"C\" in the logistic regression classier and see how it affects the accuracy and the fuzzyness of the decision boundary :-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
