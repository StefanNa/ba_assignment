{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science for Mobility / Intro to Business Analytics\n",
    "\n",
    "# Lecture 7 - Nuts and bolts of Machine Learning\n",
    "\n",
    "In this lecture, we will revisit the predictive maintenance case study that we considered in the last lecture. In doing so, we will explore some of the important Machine Learning concepts that are covered by Lecture 7 like different train/test distributions, overfitting, underfitting, regularization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, precision_score\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#matplotlib style options\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and prepare it\n",
    "\n",
    "Lets quickly load all the data and merge it in a single Pandas Dataframe. This is not the focus of today's lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from csv\n",
    "telemetry = pd.read_csv('../Week 6/telemetry.csv')\n",
    "error_count = pd.read_csv('../Week 6/error_count.csv')\n",
    "comp_rep = pd.read_csv('../Week 6/comp_rep.csv')\n",
    "machines = pd.read_csv('../Week 6/machines.csv')\n",
    "failures = pd.read_csv('../Week 6/failures.csv')\n",
    "\n",
    "# format datetime field which comes in as string\n",
    "telemetry['datetime'] = pd.to_datetime(telemetry['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "error_count['datetime'] = pd.to_datetime(error_count['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "comp_rep['datetime'] = pd.to_datetime(comp_rep['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "failures['datetime'] = pd.to_datetime(failures['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "machines['model'] = machines['model'].astype('category')\n",
    "machines = pd.get_dummies(machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data from multiple sources\n",
    "features = telemetry.merge(error_count, on=['datetime', 'machineID'], how='left')\n",
    "features = features.merge(comp_rep, on=['datetime', 'machineID'], how='left')\n",
    "features = features.merge(machines, on=['machineID'], how='left')\n",
    "\n",
    "# prepare target variable\n",
    "labeled_features = features.merge(failures, on=['datetime', 'machineID'], how='left')\n",
    "labeled_features = labeled_features.fillna(method='bfill', limit=7) # fill backward up to 24h\n",
    "labeled_features = labeled_features.fillna('none')\n",
    "\n",
    "# convert \"failure\" target variables into multiple binary targets \n",
    "# i.e. one per component indicating failure/no failure\n",
    "labeled_features['comp1_fail'] = (labeled_features['failure'] == 'comp1').astype(int)\n",
    "labeled_features['comp2_fail'] = (labeled_features['failure'] == 'comp2').astype(int)\n",
    "labeled_features['comp3_fail'] = (labeled_features['failure'] == 'comp3').astype(int)\n",
    "labeled_features['comp4_fail'] = (labeled_features['failure'] == 'comp4').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>machineID</th>\n",
       "      <th>datetime</th>\n",
       "      <th>voltmean_3h</th>\n",
       "      <th>rotatemean_3h</th>\n",
       "      <th>pressuremean_3h</th>\n",
       "      <th>vibrationmean_3h</th>\n",
       "      <th>voltsd_3h</th>\n",
       "      <th>rotatesd_3h</th>\n",
       "      <th>pressuresd_3h</th>\n",
       "      <th>vibrationsd_3h</th>\n",
       "      <th>...</th>\n",
       "      <th>age</th>\n",
       "      <th>model_model1</th>\n",
       "      <th>model_model2</th>\n",
       "      <th>model_model3</th>\n",
       "      <th>model_model4</th>\n",
       "      <th>failure</th>\n",
       "      <th>comp1_fail</th>\n",
       "      <th>comp2_fail</th>\n",
       "      <th>comp3_fail</th>\n",
       "      <th>comp4_fail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-02 06:00:00</td>\n",
       "      <td>180.133784</td>\n",
       "      <td>440.608320</td>\n",
       "      <td>94.137969</td>\n",
       "      <td>41.551544</td>\n",
       "      <td>21.322735</td>\n",
       "      <td>48.770512</td>\n",
       "      <td>2.135684</td>\n",
       "      <td>10.037208</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-02 09:00:00</td>\n",
       "      <td>176.364293</td>\n",
       "      <td>439.349655</td>\n",
       "      <td>101.553209</td>\n",
       "      <td>36.105580</td>\n",
       "      <td>18.952210</td>\n",
       "      <td>51.329636</td>\n",
       "      <td>13.789279</td>\n",
       "      <td>6.737739</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-02 12:00:00</td>\n",
       "      <td>160.384568</td>\n",
       "      <td>424.385316</td>\n",
       "      <td>99.598722</td>\n",
       "      <td>36.094637</td>\n",
       "      <td>13.047080</td>\n",
       "      <td>13.702496</td>\n",
       "      <td>9.988609</td>\n",
       "      <td>1.639962</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-02 15:00:00</td>\n",
       "      <td>170.472461</td>\n",
       "      <td>442.933997</td>\n",
       "      <td>102.380586</td>\n",
       "      <td>40.483002</td>\n",
       "      <td>16.642354</td>\n",
       "      <td>56.290447</td>\n",
       "      <td>3.305739</td>\n",
       "      <td>8.854145</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-02 18:00:00</td>\n",
       "      <td>163.263806</td>\n",
       "      <td>468.937558</td>\n",
       "      <td>102.726648</td>\n",
       "      <td>40.921802</td>\n",
       "      <td>17.424688</td>\n",
       "      <td>38.680380</td>\n",
       "      <td>9.105775</td>\n",
       "      <td>3.060781</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-02 21:00:00</td>\n",
       "      <td>163.278466</td>\n",
       "      <td>446.493166</td>\n",
       "      <td>104.387585</td>\n",
       "      <td>38.068116</td>\n",
       "      <td>21.580492</td>\n",
       "      <td>41.380958</td>\n",
       "      <td>20.725597</td>\n",
       "      <td>6.932127</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-03 00:00:00</td>\n",
       "      <td>172.191198</td>\n",
       "      <td>434.214692</td>\n",
       "      <td>93.747282</td>\n",
       "      <td>39.716482</td>\n",
       "      <td>16.369836</td>\n",
       "      <td>14.636041</td>\n",
       "      <td>18.817326</td>\n",
       "      <td>3.426997</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-03 03:00:00</td>\n",
       "      <td>175.210027</td>\n",
       "      <td>504.845430</td>\n",
       "      <td>108.512153</td>\n",
       "      <td>37.763933</td>\n",
       "      <td>5.991921</td>\n",
       "      <td>16.062702</td>\n",
       "      <td>6.382608</td>\n",
       "      <td>3.449468</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-03 06:00:00</td>\n",
       "      <td>181.690108</td>\n",
       "      <td>472.783187</td>\n",
       "      <td>93.395164</td>\n",
       "      <td>38.621099</td>\n",
       "      <td>11.514450</td>\n",
       "      <td>47.880443</td>\n",
       "      <td>2.177029</td>\n",
       "      <td>7.670520</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-03 09:00:00</td>\n",
       "      <td>172.382935</td>\n",
       "      <td>505.141261</td>\n",
       "      <td>98.524373</td>\n",
       "      <td>49.965572</td>\n",
       "      <td>7.065150</td>\n",
       "      <td>56.849540</td>\n",
       "      <td>5.230039</td>\n",
       "      <td>2.687565</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-03 12:00:00</td>\n",
       "      <td>174.303858</td>\n",
       "      <td>436.182686</td>\n",
       "      <td>94.092681</td>\n",
       "      <td>50.999589</td>\n",
       "      <td>19.017196</td>\n",
       "      <td>26.420163</td>\n",
       "      <td>7.661944</td>\n",
       "      <td>3.516734</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-03 15:00:00</td>\n",
       "      <td>176.246348</td>\n",
       "      <td>451.646684</td>\n",
       "      <td>98.102389</td>\n",
       "      <td>59.198241</td>\n",
       "      <td>12.572504</td>\n",
       "      <td>31.574383</td>\n",
       "      <td>15.559351</td>\n",
       "      <td>6.562087</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-03 18:00:00</td>\n",
       "      <td>158.433533</td>\n",
       "      <td>453.900213</td>\n",
       "      <td>98.878129</td>\n",
       "      <td>46.851925</td>\n",
       "      <td>5.136952</td>\n",
       "      <td>21.216569</td>\n",
       "      <td>11.400650</td>\n",
       "      <td>2.688559</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-03 21:00:00</td>\n",
       "      <td>162.387954</td>\n",
       "      <td>454.140377</td>\n",
       "      <td>92.651129</td>\n",
       "      <td>54.261635</td>\n",
       "      <td>4.563331</td>\n",
       "      <td>57.747656</td>\n",
       "      <td>4.754203</td>\n",
       "      <td>5.118076</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04 00:00:00</td>\n",
       "      <td>174.243192</td>\n",
       "      <td>394.998095</td>\n",
       "      <td>99.829845</td>\n",
       "      <td>46.930738</td>\n",
       "      <td>6.268730</td>\n",
       "      <td>29.167663</td>\n",
       "      <td>10.564287</td>\n",
       "      <td>6.822855</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04 03:00:00</td>\n",
       "      <td>176.443361</td>\n",
       "      <td>459.528820</td>\n",
       "      <td>111.855296</td>\n",
       "      <td>55.296056</td>\n",
       "      <td>16.330285</td>\n",
       "      <td>20.602657</td>\n",
       "      <td>7.064583</td>\n",
       "      <td>4.651468</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04 06:00:00</td>\n",
       "      <td>186.092896</td>\n",
       "      <td>451.641253</td>\n",
       "      <td>107.989359</td>\n",
       "      <td>55.308074</td>\n",
       "      <td>13.489090</td>\n",
       "      <td>62.185045</td>\n",
       "      <td>5.118176</td>\n",
       "      <td>4.904365</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04 09:00:00</td>\n",
       "      <td>166.281848</td>\n",
       "      <td>453.787824</td>\n",
       "      <td>106.187582</td>\n",
       "      <td>51.990080</td>\n",
       "      <td>24.276228</td>\n",
       "      <td>23.621315</td>\n",
       "      <td>11.176731</td>\n",
       "      <td>3.394073</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>comp4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04 12:00:00</td>\n",
       "      <td>175.412103</td>\n",
       "      <td>445.450581</td>\n",
       "      <td>100.887363</td>\n",
       "      <td>54.251534</td>\n",
       "      <td>34.918687</td>\n",
       "      <td>11.001625</td>\n",
       "      <td>10.580336</td>\n",
       "      <td>2.921501</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>comp4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04 15:00:00</td>\n",
       "      <td>157.347716</td>\n",
       "      <td>451.882075</td>\n",
       "      <td>101.289380</td>\n",
       "      <td>48.602686</td>\n",
       "      <td>24.617739</td>\n",
       "      <td>28.950883</td>\n",
       "      <td>9.966729</td>\n",
       "      <td>2.356486</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>comp4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04 18:00:00</td>\n",
       "      <td>176.450550</td>\n",
       "      <td>446.033068</td>\n",
       "      <td>84.521555</td>\n",
       "      <td>47.638836</td>\n",
       "      <td>8.071400</td>\n",
       "      <td>76.511343</td>\n",
       "      <td>2.636879</td>\n",
       "      <td>4.108621</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>comp4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04 21:00:00</td>\n",
       "      <td>190.325814</td>\n",
       "      <td>422.692565</td>\n",
       "      <td>107.393234</td>\n",
       "      <td>49.552856</td>\n",
       "      <td>8.390777</td>\n",
       "      <td>7.176553</td>\n",
       "      <td>4.262645</td>\n",
       "      <td>7.598552</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>comp4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-05 00:00:00</td>\n",
       "      <td>169.985134</td>\n",
       "      <td>458.929418</td>\n",
       "      <td>91.494362</td>\n",
       "      <td>54.882021</td>\n",
       "      <td>9.451483</td>\n",
       "      <td>12.052752</td>\n",
       "      <td>3.685906</td>\n",
       "      <td>6.621183</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>comp4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-05 03:00:00</td>\n",
       "      <td>149.082619</td>\n",
       "      <td>412.180336</td>\n",
       "      <td>93.509785</td>\n",
       "      <td>54.386079</td>\n",
       "      <td>19.075952</td>\n",
       "      <td>30.715081</td>\n",
       "      <td>3.090266</td>\n",
       "      <td>6.530610</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>comp4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-05 06:00:00</td>\n",
       "      <td>185.782709</td>\n",
       "      <td>439.531288</td>\n",
       "      <td>99.413660</td>\n",
       "      <td>51.558082</td>\n",
       "      <td>14.495664</td>\n",
       "      <td>45.663743</td>\n",
       "      <td>4.289212</td>\n",
       "      <td>7.330397</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>comp4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-05 09:00:00</td>\n",
       "      <td>169.084809</td>\n",
       "      <td>463.433785</td>\n",
       "      <td>107.678774</td>\n",
       "      <td>41.710336</td>\n",
       "      <td>12.245544</td>\n",
       "      <td>61.759107</td>\n",
       "      <td>4.400233</td>\n",
       "      <td>9.750017</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-05 12:00:00</td>\n",
       "      <td>165.518790</td>\n",
       "      <td>449.743255</td>\n",
       "      <td>110.377851</td>\n",
       "      <td>38.952082</td>\n",
       "      <td>23.170638</td>\n",
       "      <td>45.762142</td>\n",
       "      <td>14.009473</td>\n",
       "      <td>0.797364</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-05 15:00:00</td>\n",
       "      <td>175.989642</td>\n",
       "      <td>419.863490</td>\n",
       "      <td>112.571146</td>\n",
       "      <td>41.514254</td>\n",
       "      <td>4.028327</td>\n",
       "      <td>20.148499</td>\n",
       "      <td>5.862629</td>\n",
       "      <td>9.702498</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-05 18:00:00</td>\n",
       "      <td>188.576444</td>\n",
       "      <td>487.336742</td>\n",
       "      <td>88.967297</td>\n",
       "      <td>36.571052</td>\n",
       "      <td>8.278605</td>\n",
       "      <td>76.534023</td>\n",
       "      <td>11.892088</td>\n",
       "      <td>1.945849</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-05 21:00:00</td>\n",
       "      <td>166.681364</td>\n",
       "      <td>481.685320</td>\n",
       "      <td>104.154110</td>\n",
       "      <td>38.662638</td>\n",
       "      <td>11.957697</td>\n",
       "      <td>25.052743</td>\n",
       "      <td>11.999161</td>\n",
       "      <td>4.804263</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290612</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-02 06:00:00</td>\n",
       "      <td>165.259415</td>\n",
       "      <td>432.364050</td>\n",
       "      <td>96.793097</td>\n",
       "      <td>38.697882</td>\n",
       "      <td>16.715588</td>\n",
       "      <td>9.197585</td>\n",
       "      <td>11.016730</td>\n",
       "      <td>9.167743</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290613</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-02 09:00:00</td>\n",
       "      <td>185.907346</td>\n",
       "      <td>465.062411</td>\n",
       "      <td>94.161434</td>\n",
       "      <td>36.156060</td>\n",
       "      <td>22.822289</td>\n",
       "      <td>64.351154</td>\n",
       "      <td>6.469484</td>\n",
       "      <td>1.656610</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290614</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-02 12:00:00</td>\n",
       "      <td>167.546991</td>\n",
       "      <td>448.203119</td>\n",
       "      <td>99.383591</td>\n",
       "      <td>39.659572</td>\n",
       "      <td>2.573507</td>\n",
       "      <td>84.299208</td>\n",
       "      <td>2.490792</td>\n",
       "      <td>2.252574</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290615</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-02 15:00:00</td>\n",
       "      <td>175.468904</td>\n",
       "      <td>441.861941</td>\n",
       "      <td>105.814802</td>\n",
       "      <td>38.788653</td>\n",
       "      <td>9.104554</td>\n",
       "      <td>48.615069</td>\n",
       "      <td>6.004070</td>\n",
       "      <td>3.244295</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290616</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-02 18:00:00</td>\n",
       "      <td>157.401371</td>\n",
       "      <td>459.332121</td>\n",
       "      <td>93.247465</td>\n",
       "      <td>42.236723</td>\n",
       "      <td>14.711827</td>\n",
       "      <td>45.268580</td>\n",
       "      <td>5.590642</td>\n",
       "      <td>2.204472</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290617</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-02 21:00:00</td>\n",
       "      <td>168.651510</td>\n",
       "      <td>430.056138</td>\n",
       "      <td>104.487324</td>\n",
       "      <td>35.735005</td>\n",
       "      <td>16.328969</td>\n",
       "      <td>45.108180</td>\n",
       "      <td>9.103806</td>\n",
       "      <td>6.093867</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290618</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-03 00:00:00</td>\n",
       "      <td>168.623762</td>\n",
       "      <td>497.504580</td>\n",
       "      <td>100.682235</td>\n",
       "      <td>40.610939</td>\n",
       "      <td>12.914771</td>\n",
       "      <td>25.775781</td>\n",
       "      <td>11.444951</td>\n",
       "      <td>3.359673</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290619</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-03 03:00:00</td>\n",
       "      <td>168.537058</td>\n",
       "      <td>441.837105</td>\n",
       "      <td>87.893111</td>\n",
       "      <td>40.076219</td>\n",
       "      <td>23.866338</td>\n",
       "      <td>36.817201</td>\n",
       "      <td>16.820180</td>\n",
       "      <td>0.482728</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290620</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-03 06:00:00</td>\n",
       "      <td>161.436752</td>\n",
       "      <td>401.579802</td>\n",
       "      <td>90.792431</td>\n",
       "      <td>35.624101</td>\n",
       "      <td>14.429283</td>\n",
       "      <td>85.801834</td>\n",
       "      <td>16.225371</td>\n",
       "      <td>1.396074</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290621</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-03 09:00:00</td>\n",
       "      <td>188.559785</td>\n",
       "      <td>491.929571</td>\n",
       "      <td>102.821662</td>\n",
       "      <td>40.034460</td>\n",
       "      <td>12.668164</td>\n",
       "      <td>41.768856</td>\n",
       "      <td>9.448383</td>\n",
       "      <td>8.454640</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290622</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-03 12:00:00</td>\n",
       "      <td>164.149847</td>\n",
       "      <td>466.463280</td>\n",
       "      <td>100.447025</td>\n",
       "      <td>40.694147</td>\n",
       "      <td>16.460088</td>\n",
       "      <td>31.589198</td>\n",
       "      <td>10.188202</td>\n",
       "      <td>5.086589</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290623</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-03 15:00:00</td>\n",
       "      <td>184.727094</td>\n",
       "      <td>487.140570</td>\n",
       "      <td>100.860907</td>\n",
       "      <td>36.430834</td>\n",
       "      <td>12.401664</td>\n",
       "      <td>57.508894</td>\n",
       "      <td>18.394116</td>\n",
       "      <td>7.153156</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290624</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-03 18:00:00</td>\n",
       "      <td>171.585447</td>\n",
       "      <td>479.021432</td>\n",
       "      <td>101.846824</td>\n",
       "      <td>49.115340</td>\n",
       "      <td>13.318878</td>\n",
       "      <td>40.471453</td>\n",
       "      <td>10.951704</td>\n",
       "      <td>0.649491</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290625</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-03 21:00:00</td>\n",
       "      <td>162.144446</td>\n",
       "      <td>404.865418</td>\n",
       "      <td>98.384468</td>\n",
       "      <td>35.389856</td>\n",
       "      <td>22.516178</td>\n",
       "      <td>36.216388</td>\n",
       "      <td>11.492089</td>\n",
       "      <td>7.269283</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290626</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-04 00:00:00</td>\n",
       "      <td>166.584930</td>\n",
       "      <td>437.980304</td>\n",
       "      <td>104.019479</td>\n",
       "      <td>43.766793</td>\n",
       "      <td>22.109109</td>\n",
       "      <td>65.256390</td>\n",
       "      <td>12.621841</td>\n",
       "      <td>1.793100</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290627</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-04 03:00:00</td>\n",
       "      <td>173.182209</td>\n",
       "      <td>452.585928</td>\n",
       "      <td>106.572235</td>\n",
       "      <td>40.534601</td>\n",
       "      <td>16.930726</td>\n",
       "      <td>38.788180</td>\n",
       "      <td>10.747137</td>\n",
       "      <td>6.510290</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290628</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-04 06:00:00</td>\n",
       "      <td>155.554082</td>\n",
       "      <td>464.175866</td>\n",
       "      <td>102.615428</td>\n",
       "      <td>36.003311</td>\n",
       "      <td>7.678204</td>\n",
       "      <td>24.248612</td>\n",
       "      <td>6.064152</td>\n",
       "      <td>5.007039</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290629</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-04 09:00:00</td>\n",
       "      <td>163.814555</td>\n",
       "      <td>433.614467</td>\n",
       "      <td>114.798438</td>\n",
       "      <td>36.454615</td>\n",
       "      <td>5.259901</td>\n",
       "      <td>40.947023</td>\n",
       "      <td>10.677648</td>\n",
       "      <td>8.252193</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290630</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-04 12:00:00</td>\n",
       "      <td>169.196188</td>\n",
       "      <td>403.488184</td>\n",
       "      <td>94.199431</td>\n",
       "      <td>39.189491</td>\n",
       "      <td>22.977467</td>\n",
       "      <td>27.176467</td>\n",
       "      <td>9.430194</td>\n",
       "      <td>13.841831</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290631</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-04 15:00:00</td>\n",
       "      <td>165.814250</td>\n",
       "      <td>446.765824</td>\n",
       "      <td>99.334107</td>\n",
       "      <td>44.464271</td>\n",
       "      <td>1.457549</td>\n",
       "      <td>58.086715</td>\n",
       "      <td>1.622380</td>\n",
       "      <td>3.173978</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290632</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-04 18:00:00</td>\n",
       "      <td>167.848340</td>\n",
       "      <td>438.393471</td>\n",
       "      <td>90.054937</td>\n",
       "      <td>40.301288</td>\n",
       "      <td>15.958412</td>\n",
       "      <td>40.168662</td>\n",
       "      <td>11.238292</td>\n",
       "      <td>3.633503</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290633</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-04 21:00:00</td>\n",
       "      <td>173.508300</td>\n",
       "      <td>439.917848</td>\n",
       "      <td>93.063793</td>\n",
       "      <td>38.750136</td>\n",
       "      <td>7.633479</td>\n",
       "      <td>44.399657</td>\n",
       "      <td>11.019912</td>\n",
       "      <td>4.952713</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290634</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-05 00:00:00</td>\n",
       "      <td>182.432617</td>\n",
       "      <td>497.264899</td>\n",
       "      <td>95.443869</td>\n",
       "      <td>40.594815</td>\n",
       "      <td>9.940475</td>\n",
       "      <td>77.558997</td>\n",
       "      <td>4.707020</td>\n",
       "      <td>3.106529</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290635</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-05 03:00:00</td>\n",
       "      <td>158.783988</td>\n",
       "      <td>438.405164</td>\n",
       "      <td>100.420803</td>\n",
       "      <td>40.153025</td>\n",
       "      <td>10.849108</td>\n",
       "      <td>72.556330</td>\n",
       "      <td>2.576581</td>\n",
       "      <td>4.504970</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290636</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-05 06:00:00</td>\n",
       "      <td>183.150826</td>\n",
       "      <td>426.209117</td>\n",
       "      <td>98.880399</td>\n",
       "      <td>34.418557</td>\n",
       "      <td>20.539063</td>\n",
       "      <td>29.605169</td>\n",
       "      <td>13.588936</td>\n",
       "      <td>7.168643</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290637</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-05 09:00:00</td>\n",
       "      <td>188.267556</td>\n",
       "      <td>407.256175</td>\n",
       "      <td>108.931184</td>\n",
       "      <td>36.553233</td>\n",
       "      <td>9.599915</td>\n",
       "      <td>40.722980</td>\n",
       "      <td>1.639521</td>\n",
       "      <td>5.724500</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290638</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-05 12:00:00</td>\n",
       "      <td>167.859576</td>\n",
       "      <td>465.992407</td>\n",
       "      <td>107.953155</td>\n",
       "      <td>42.708899</td>\n",
       "      <td>14.190347</td>\n",
       "      <td>92.277799</td>\n",
       "      <td>9.577243</td>\n",
       "      <td>0.735339</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290639</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-05 15:00:00</td>\n",
       "      <td>170.348099</td>\n",
       "      <td>434.234744</td>\n",
       "      <td>104.514343</td>\n",
       "      <td>38.607950</td>\n",
       "      <td>10.232598</td>\n",
       "      <td>49.524471</td>\n",
       "      <td>12.445345</td>\n",
       "      <td>2.596743</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290640</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-05 18:00:00</td>\n",
       "      <td>152.265370</td>\n",
       "      <td>459.557611</td>\n",
       "      <td>103.536524</td>\n",
       "      <td>40.718426</td>\n",
       "      <td>6.758667</td>\n",
       "      <td>27.051145</td>\n",
       "      <td>12.824247</td>\n",
       "      <td>2.752883</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290641</th>\n",
       "      <td>100</td>\n",
       "      <td>2015-10-05 21:00:00</td>\n",
       "      <td>162.887965</td>\n",
       "      <td>481.415205</td>\n",
       "      <td>96.687092</td>\n",
       "      <td>37.162591</td>\n",
       "      <td>20.541773</td>\n",
       "      <td>55.057460</td>\n",
       "      <td>11.713728</td>\n",
       "      <td>3.539798</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>290642 rows  37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        machineID            datetime  voltmean_3h  rotatemean_3h  \\\n",
       "0               1 2015-01-02 06:00:00   180.133784     440.608320   \n",
       "1               1 2015-01-02 09:00:00   176.364293     439.349655   \n",
       "2               1 2015-01-02 12:00:00   160.384568     424.385316   \n",
       "3               1 2015-01-02 15:00:00   170.472461     442.933997   \n",
       "4               1 2015-01-02 18:00:00   163.263806     468.937558   \n",
       "5               1 2015-01-02 21:00:00   163.278466     446.493166   \n",
       "6               1 2015-01-03 00:00:00   172.191198     434.214692   \n",
       "7               1 2015-01-03 03:00:00   175.210027     504.845430   \n",
       "8               1 2015-01-03 06:00:00   181.690108     472.783187   \n",
       "9               1 2015-01-03 09:00:00   172.382935     505.141261   \n",
       "10              1 2015-01-03 12:00:00   174.303858     436.182686   \n",
       "11              1 2015-01-03 15:00:00   176.246348     451.646684   \n",
       "12              1 2015-01-03 18:00:00   158.433533     453.900213   \n",
       "13              1 2015-01-03 21:00:00   162.387954     454.140377   \n",
       "14              1 2015-01-04 00:00:00   174.243192     394.998095   \n",
       "15              1 2015-01-04 03:00:00   176.443361     459.528820   \n",
       "16              1 2015-01-04 06:00:00   186.092896     451.641253   \n",
       "17              1 2015-01-04 09:00:00   166.281848     453.787824   \n",
       "18              1 2015-01-04 12:00:00   175.412103     445.450581   \n",
       "19              1 2015-01-04 15:00:00   157.347716     451.882075   \n",
       "20              1 2015-01-04 18:00:00   176.450550     446.033068   \n",
       "21              1 2015-01-04 21:00:00   190.325814     422.692565   \n",
       "22              1 2015-01-05 00:00:00   169.985134     458.929418   \n",
       "23              1 2015-01-05 03:00:00   149.082619     412.180336   \n",
       "24              1 2015-01-05 06:00:00   185.782709     439.531288   \n",
       "25              1 2015-01-05 09:00:00   169.084809     463.433785   \n",
       "26              1 2015-01-05 12:00:00   165.518790     449.743255   \n",
       "27              1 2015-01-05 15:00:00   175.989642     419.863490   \n",
       "28              1 2015-01-05 18:00:00   188.576444     487.336742   \n",
       "29              1 2015-01-05 21:00:00   166.681364     481.685320   \n",
       "...           ...                 ...          ...            ...   \n",
       "290612        100 2015-10-02 06:00:00   165.259415     432.364050   \n",
       "290613        100 2015-10-02 09:00:00   185.907346     465.062411   \n",
       "290614        100 2015-10-02 12:00:00   167.546991     448.203119   \n",
       "290615        100 2015-10-02 15:00:00   175.468904     441.861941   \n",
       "290616        100 2015-10-02 18:00:00   157.401371     459.332121   \n",
       "290617        100 2015-10-02 21:00:00   168.651510     430.056138   \n",
       "290618        100 2015-10-03 00:00:00   168.623762     497.504580   \n",
       "290619        100 2015-10-03 03:00:00   168.537058     441.837105   \n",
       "290620        100 2015-10-03 06:00:00   161.436752     401.579802   \n",
       "290621        100 2015-10-03 09:00:00   188.559785     491.929571   \n",
       "290622        100 2015-10-03 12:00:00   164.149847     466.463280   \n",
       "290623        100 2015-10-03 15:00:00   184.727094     487.140570   \n",
       "290624        100 2015-10-03 18:00:00   171.585447     479.021432   \n",
       "290625        100 2015-10-03 21:00:00   162.144446     404.865418   \n",
       "290626        100 2015-10-04 00:00:00   166.584930     437.980304   \n",
       "290627        100 2015-10-04 03:00:00   173.182209     452.585928   \n",
       "290628        100 2015-10-04 06:00:00   155.554082     464.175866   \n",
       "290629        100 2015-10-04 09:00:00   163.814555     433.614467   \n",
       "290630        100 2015-10-04 12:00:00   169.196188     403.488184   \n",
       "290631        100 2015-10-04 15:00:00   165.814250     446.765824   \n",
       "290632        100 2015-10-04 18:00:00   167.848340     438.393471   \n",
       "290633        100 2015-10-04 21:00:00   173.508300     439.917848   \n",
       "290634        100 2015-10-05 00:00:00   182.432617     497.264899   \n",
       "290635        100 2015-10-05 03:00:00   158.783988     438.405164   \n",
       "290636        100 2015-10-05 06:00:00   183.150826     426.209117   \n",
       "290637        100 2015-10-05 09:00:00   188.267556     407.256175   \n",
       "290638        100 2015-10-05 12:00:00   167.859576     465.992407   \n",
       "290639        100 2015-10-05 15:00:00   170.348099     434.234744   \n",
       "290640        100 2015-10-05 18:00:00   152.265370     459.557611   \n",
       "290641        100 2015-10-05 21:00:00   162.887965     481.415205   \n",
       "\n",
       "        pressuremean_3h  vibrationmean_3h  voltsd_3h  rotatesd_3h  \\\n",
       "0             94.137969         41.551544  21.322735    48.770512   \n",
       "1            101.553209         36.105580  18.952210    51.329636   \n",
       "2             99.598722         36.094637  13.047080    13.702496   \n",
       "3            102.380586         40.483002  16.642354    56.290447   \n",
       "4            102.726648         40.921802  17.424688    38.680380   \n",
       "5            104.387585         38.068116  21.580492    41.380958   \n",
       "6             93.747282         39.716482  16.369836    14.636041   \n",
       "7            108.512153         37.763933   5.991921    16.062702   \n",
       "8             93.395164         38.621099  11.514450    47.880443   \n",
       "9             98.524373         49.965572   7.065150    56.849540   \n",
       "10            94.092681         50.999589  19.017196    26.420163   \n",
       "11            98.102389         59.198241  12.572504    31.574383   \n",
       "12            98.878129         46.851925   5.136952    21.216569   \n",
       "13            92.651129         54.261635   4.563331    57.747656   \n",
       "14            99.829845         46.930738   6.268730    29.167663   \n",
       "15           111.855296         55.296056  16.330285    20.602657   \n",
       "16           107.989359         55.308074  13.489090    62.185045   \n",
       "17           106.187582         51.990080  24.276228    23.621315   \n",
       "18           100.887363         54.251534  34.918687    11.001625   \n",
       "19           101.289380         48.602686  24.617739    28.950883   \n",
       "20            84.521555         47.638836   8.071400    76.511343   \n",
       "21           107.393234         49.552856   8.390777     7.176553   \n",
       "22            91.494362         54.882021   9.451483    12.052752   \n",
       "23            93.509785         54.386079  19.075952    30.715081   \n",
       "24            99.413660         51.558082  14.495664    45.663743   \n",
       "25           107.678774         41.710336  12.245544    61.759107   \n",
       "26           110.377851         38.952082  23.170638    45.762142   \n",
       "27           112.571146         41.514254   4.028327    20.148499   \n",
       "28            88.967297         36.571052   8.278605    76.534023   \n",
       "29           104.154110         38.662638  11.957697    25.052743   \n",
       "...                 ...               ...        ...          ...   \n",
       "290612        96.793097         38.697882  16.715588     9.197585   \n",
       "290613        94.161434         36.156060  22.822289    64.351154   \n",
       "290614        99.383591         39.659572   2.573507    84.299208   \n",
       "290615       105.814802         38.788653   9.104554    48.615069   \n",
       "290616        93.247465         42.236723  14.711827    45.268580   \n",
       "290617       104.487324         35.735005  16.328969    45.108180   \n",
       "290618       100.682235         40.610939  12.914771    25.775781   \n",
       "290619        87.893111         40.076219  23.866338    36.817201   \n",
       "290620        90.792431         35.624101  14.429283    85.801834   \n",
       "290621       102.821662         40.034460  12.668164    41.768856   \n",
       "290622       100.447025         40.694147  16.460088    31.589198   \n",
       "290623       100.860907         36.430834  12.401664    57.508894   \n",
       "290624       101.846824         49.115340  13.318878    40.471453   \n",
       "290625        98.384468         35.389856  22.516178    36.216388   \n",
       "290626       104.019479         43.766793  22.109109    65.256390   \n",
       "290627       106.572235         40.534601  16.930726    38.788180   \n",
       "290628       102.615428         36.003311   7.678204    24.248612   \n",
       "290629       114.798438         36.454615   5.259901    40.947023   \n",
       "290630        94.199431         39.189491  22.977467    27.176467   \n",
       "290631        99.334107         44.464271   1.457549    58.086715   \n",
       "290632        90.054937         40.301288  15.958412    40.168662   \n",
       "290633        93.063793         38.750136   7.633479    44.399657   \n",
       "290634        95.443869         40.594815   9.940475    77.558997   \n",
       "290635       100.420803         40.153025  10.849108    72.556330   \n",
       "290636        98.880399         34.418557  20.539063    29.605169   \n",
       "290637       108.931184         36.553233   9.599915    40.722980   \n",
       "290638       107.953155         42.708899  14.190347    92.277799   \n",
       "290639       104.514343         38.607950  10.232598    49.524471   \n",
       "290640       103.536524         40.718426   6.758667    27.051145   \n",
       "290641        96.687092         37.162591  20.541773    55.057460   \n",
       "\n",
       "        pressuresd_3h  vibrationsd_3h  ...  age  model_model1  model_model2  \\\n",
       "0            2.135684       10.037208  ...   18             0             0   \n",
       "1           13.789279        6.737739  ...   18             0             0   \n",
       "2            9.988609        1.639962  ...   18             0             0   \n",
       "3            3.305739        8.854145  ...   18             0             0   \n",
       "4            9.105775        3.060781  ...   18             0             0   \n",
       "5           20.725597        6.932127  ...   18             0             0   \n",
       "6           18.817326        3.426997  ...   18             0             0   \n",
       "7            6.382608        3.449468  ...   18             0             0   \n",
       "8            2.177029        7.670520  ...   18             0             0   \n",
       "9            5.230039        2.687565  ...   18             0             0   \n",
       "10           7.661944        3.516734  ...   18             0             0   \n",
       "11          15.559351        6.562087  ...   18             0             0   \n",
       "12          11.400650        2.688559  ...   18             0             0   \n",
       "13           4.754203        5.118076  ...   18             0             0   \n",
       "14          10.564287        6.822855  ...   18             0             0   \n",
       "15           7.064583        4.651468  ...   18             0             0   \n",
       "16           5.118176        4.904365  ...   18             0             0   \n",
       "17          11.176731        3.394073  ...   18             0             0   \n",
       "18          10.580336        2.921501  ...   18             0             0   \n",
       "19           9.966729        2.356486  ...   18             0             0   \n",
       "20           2.636879        4.108621  ...   18             0             0   \n",
       "21           4.262645        7.598552  ...   18             0             0   \n",
       "22           3.685906        6.621183  ...   18             0             0   \n",
       "23           3.090266        6.530610  ...   18             0             0   \n",
       "24           4.289212        7.330397  ...   18             0             0   \n",
       "25           4.400233        9.750017  ...   18             0             0   \n",
       "26          14.009473        0.797364  ...   18             0             0   \n",
       "27           5.862629        9.702498  ...   18             0             0   \n",
       "28          11.892088        1.945849  ...   18             0             0   \n",
       "29          11.999161        4.804263  ...   18             0             0   \n",
       "...               ...             ...  ...  ...           ...           ...   \n",
       "290612      11.016730        9.167743  ...    5             0             0   \n",
       "290613       6.469484        1.656610  ...    5             0             0   \n",
       "290614       2.490792        2.252574  ...    5             0             0   \n",
       "290615       6.004070        3.244295  ...    5             0             0   \n",
       "290616       5.590642        2.204472  ...    5             0             0   \n",
       "290617       9.103806        6.093867  ...    5             0             0   \n",
       "290618      11.444951        3.359673  ...    5             0             0   \n",
       "290619      16.820180        0.482728  ...    5             0             0   \n",
       "290620      16.225371        1.396074  ...    5             0             0   \n",
       "290621       9.448383        8.454640  ...    5             0             0   \n",
       "290622      10.188202        5.086589  ...    5             0             0   \n",
       "290623      18.394116        7.153156  ...    5             0             0   \n",
       "290624      10.951704        0.649491  ...    5             0             0   \n",
       "290625      11.492089        7.269283  ...    5             0             0   \n",
       "290626      12.621841        1.793100  ...    5             0             0   \n",
       "290627      10.747137        6.510290  ...    5             0             0   \n",
       "290628       6.064152        5.007039  ...    5             0             0   \n",
       "290629      10.677648        8.252193  ...    5             0             0   \n",
       "290630       9.430194       13.841831  ...    5             0             0   \n",
       "290631       1.622380        3.173978  ...    5             0             0   \n",
       "290632      11.238292        3.633503  ...    5             0             0   \n",
       "290633      11.019912        4.952713  ...    5             0             0   \n",
       "290634       4.707020        3.106529  ...    5             0             0   \n",
       "290635       2.576581        4.504970  ...    5             0             0   \n",
       "290636      13.588936        7.168643  ...    5             0             0   \n",
       "290637       1.639521        5.724500  ...    5             0             0   \n",
       "290638       9.577243        0.735339  ...    5             0             0   \n",
       "290639      12.445345        2.596743  ...    5             0             0   \n",
       "290640      12.824247        2.752883  ...    5             0             0   \n",
       "290641      11.713728        3.539798  ...    5             0             0   \n",
       "\n",
       "        model_model3  model_model4  failure  comp1_fail  comp2_fail  \\\n",
       "0                  1             0     none           0           0   \n",
       "1                  1             0     none           0           0   \n",
       "2                  1             0     none           0           0   \n",
       "3                  1             0     none           0           0   \n",
       "4                  1             0     none           0           0   \n",
       "5                  1             0     none           0           0   \n",
       "6                  1             0     none           0           0   \n",
       "7                  1             0     none           0           0   \n",
       "8                  1             0     none           0           0   \n",
       "9                  1             0     none           0           0   \n",
       "10                 1             0     none           0           0   \n",
       "11                 1             0     none           0           0   \n",
       "12                 1             0     none           0           0   \n",
       "13                 1             0     none           0           0   \n",
       "14                 1             0     none           0           0   \n",
       "15                 1             0     none           0           0   \n",
       "16                 1             0     none           0           0   \n",
       "17                 1             0    comp4           0           0   \n",
       "18                 1             0    comp4           0           0   \n",
       "19                 1             0    comp4           0           0   \n",
       "20                 1             0    comp4           0           0   \n",
       "21                 1             0    comp4           0           0   \n",
       "22                 1             0    comp4           0           0   \n",
       "23                 1             0    comp4           0           0   \n",
       "24                 1             0    comp4           0           0   \n",
       "25                 1             0     none           0           0   \n",
       "26                 1             0     none           0           0   \n",
       "27                 1             0     none           0           0   \n",
       "28                 1             0     none           0           0   \n",
       "29                 1             0     none           0           0   \n",
       "...              ...           ...      ...         ...         ...   \n",
       "290612             0             1     none           0           0   \n",
       "290613             0             1     none           0           0   \n",
       "290614             0             1     none           0           0   \n",
       "290615             0             1     none           0           0   \n",
       "290616             0             1     none           0           0   \n",
       "290617             0             1     none           0           0   \n",
       "290618             0             1     none           0           0   \n",
       "290619             0             1     none           0           0   \n",
       "290620             0             1     none           0           0   \n",
       "290621             0             1     none           0           0   \n",
       "290622             0             1     none           0           0   \n",
       "290623             0             1     none           0           0   \n",
       "290624             0             1     none           0           0   \n",
       "290625             0             1     none           0           0   \n",
       "290626             0             1     none           0           0   \n",
       "290627             0             1     none           0           0   \n",
       "290628             0             1     none           0           0   \n",
       "290629             0             1     none           0           0   \n",
       "290630             0             1     none           0           0   \n",
       "290631             0             1     none           0           0   \n",
       "290632             0             1     none           0           0   \n",
       "290633             0             1     none           0           0   \n",
       "290634             0             1     none           0           0   \n",
       "290635             0             1     none           0           0   \n",
       "290636             0             1     none           0           0   \n",
       "290637             0             1     none           0           0   \n",
       "290638             0             1     none           0           0   \n",
       "290639             0             1     none           0           0   \n",
       "290640             0             1     none           0           0   \n",
       "290641             0             1     none           0           0   \n",
       "\n",
       "        comp3_fail  comp4_fail  \n",
       "0                0           0  \n",
       "1                0           0  \n",
       "2                0           0  \n",
       "3                0           0  \n",
       "4                0           0  \n",
       "5                0           0  \n",
       "6                0           0  \n",
       "7                0           0  \n",
       "8                0           0  \n",
       "9                0           0  \n",
       "10               0           0  \n",
       "11               0           0  \n",
       "12               0           0  \n",
       "13               0           0  \n",
       "14               0           0  \n",
       "15               0           0  \n",
       "16               0           0  \n",
       "17               0           1  \n",
       "18               0           1  \n",
       "19               0           1  \n",
       "20               0           1  \n",
       "21               0           1  \n",
       "22               0           1  \n",
       "23               0           1  \n",
       "24               0           1  \n",
       "25               0           0  \n",
       "26               0           0  \n",
       "27               0           0  \n",
       "28               0           0  \n",
       "29               0           0  \n",
       "...            ...         ...  \n",
       "290612           0           0  \n",
       "290613           0           0  \n",
       "290614           0           0  \n",
       "290615           0           0  \n",
       "290616           0           0  \n",
       "290617           0           0  \n",
       "290618           0           0  \n",
       "290619           0           0  \n",
       "290620           0           0  \n",
       "290621           0           0  \n",
       "290622           0           0  \n",
       "290623           0           0  \n",
       "290624           0           0  \n",
       "290625           0           0  \n",
       "290626           0           0  \n",
       "290627           0           0  \n",
       "290628           0           0  \n",
       "290629           0           0  \n",
       "290630           0           0  \n",
       "290631           0           0  \n",
       "290632           0           0  \n",
       "290633           0           0  \n",
       "290634           0           0  \n",
       "290635           0           0  \n",
       "290636           0           0  \n",
       "290637           0           0  \n",
       "290638           0           0  \n",
       "290639           0           0  \n",
       "290640           0           0  \n",
       "290641           0           0  \n",
       "\n",
       "[290642 rows x 37 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define features that we will be using for training the classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_use = ['voltmean_3h', 'rotatemean_3h', 'pressuremean_3h', 'vibrationmean_3h', # telemetry features (3h)\n",
    "                   'voltsd_3h', 'rotatesd_3h', 'pressuresd_3h', 'vibrationsd_3h', \n",
    "                   'voltmean_24h', 'rotatemean_24h', 'pressuremean_24h', 'vibrationmean_24h', # telemetry feat (24h)\n",
    "                   'voltsd_24h', 'rotatesd_24h', 'pressuresd_24h', 'vibrationsd_24h', \n",
    "                   'error1count', 'error2count', 'error3count', 'error4count', 'error5count', # errors over last 24h\n",
    "                   'comp1', 'comp2', 'comp3', 'comp4', # days since last component replacement\n",
    "                   'model_model1', 'model_model2', 'model_model3', 'model_model4', 'age'] # machine characteristics\n",
    "\n",
    "target_variables = 'comp1_fail' # whether or not component 1 will fail in the next 24h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Different train/test distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you still remember the way we splitted the dataset into train, validation and test sets? \n",
    "\n",
    "<div>\n",
    "    <br/>\n",
    "    <img src=\"http://mlsm.man.dtu.dk/wp-content/uploads/2019/10/val_set.png\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "Recall that we set aside a validation set that we used for coming up with a good neural network architecture and for performing hyper-parameter tuning. This is how we split the data:\n",
    "\n",
    "- Train set: all data until 2015-05-31 01:00:00\n",
    "\n",
    "- Validation set: data between 2015-06-01 01:00:00 and 2015-07-31 01:00:00\n",
    "\n",
    "- Test set: all data after 2015-08-01 01:00:00\n",
    "\n",
    "Let's consider this split again, create the different train, validation and test sets, and standardize the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train examples: 119117\n",
      "Num validation examples: 48005\n",
      "Num test examples: 121920\n"
     ]
    }
   ],
   "source": [
    "# define cutoff between trainset and testset\n",
    "trainset_end = pd.to_datetime('2015-05-31 01:00:00')\n",
    "validationset_start = pd.to_datetime('2015-06-01 01:00:00')\n",
    "validationset_end = pd.to_datetime('2015-07-31 01:00:00')\n",
    "testset_start = pd.to_datetime('2015-08-01 01:00:00')\n",
    "\n",
    "# train/val/test split\n",
    "X_train = labeled_features.loc[labeled_features['datetime'] < trainset_end, features_to_use]\n",
    "y_train = labeled_features.loc[labeled_features['datetime'] < trainset_end, target_variables]\n",
    "X_val = labeled_features.loc[((labeled_features['datetime'] > validationset_start) & (labeled_features['datetime'] < validationset_end)), features_to_use]\n",
    "y_val = labeled_features.loc[((labeled_features['datetime'] > validationset_start) & (labeled_features['datetime'] < validationset_end)), target_variables]\n",
    "X_test = labeled_features.loc[labeled_features['datetime'] > testset_start, features_to_use]\n",
    "y_test = labeled_features.loc[labeled_features['datetime'] > testset_start, target_variables]\n",
    "\n",
    "print(\"Num train examples:\", len(y_train))\n",
    "print(\"Num validation examples:\", len(y_val))\n",
    "print(\"Num test examples:\", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that so far we haven't standardized the data. Let's first see how that can affect the learned classifiers.\n",
    "\n",
    "Function to evaluate predictions from last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to evaluate predictions\n",
    "def evaluate(y_true, y_pred, print_cm=False):\n",
    "    # calculate and display confusion matrix\n",
    "    labels = np.unique(y_true)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    if print_cm:\n",
    "        print('Confusion matrix\\n- x-axis is true labels (none, comp1, etc.)\\n- y-axis is predicted labels')\n",
    "        print(cm)\n",
    "\n",
    "    # calculate precision, recall, and F1 score\n",
    "    accuracy = float(np.trace(cm)) / np.sum(cm)\n",
    "    precision = precision_score(y_true, y_pred, average=None, labels=labels)[1]\n",
    "    recall = recall_score(y_true, y_pred, average=None, labels=labels)[1]\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print(\"accuracy:\", accuracy)\n",
    "    print(\"precision:\", precision)\n",
    "    print(\"recall:\", recall)\n",
    "    print(\"f1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now take a logistic regression model as an example, fit it and evaluate it on the **unstandardized** train and test sets. Note that, since it is not the focus of this notebook, in this case, we are not using the validation set to tune any hyper-parameters of the model. However, normally, you should do that. And make sure that you always do it using the validation set and not the test set! :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stef/Documents/ba_intro/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set results:\n",
      "accuracy: 0.9986819681489628\n",
      "precision: 0.8729446935724963\n",
      "recall: 0.8902439024390244\n",
      "f1 score: 0.8815094339622642\n",
      "- Test set results:\n",
      "accuracy: 0.9980396981627296\n",
      "precision: 0.8466819221967964\n",
      "recall: 0.6826568265682657\n",
      "f1 score: 0.7558733401430031\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# estimate model on trainset\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print(\"- Train set results:\")\n",
    "evaluate(y_train, y_pred_train)\n",
    "print(\"- Test set results:\")\n",
    "evaluate(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's now have a look at the version with the standardized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data\n",
    "X_mean = X_train.mean(axis=0)\n",
    "X_std = X_train.std(axis=0)\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_val = (X_val - X_mean) / X_std\n",
    "X_test = (X_test - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stef/Documents/ba_intro/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set results:\n",
      "accuracy: 0.9989422164762376\n",
      "precision: 0.8764204545454546\n",
      "recall: 0.9405487804878049\n",
      "f1 score: 0.9073529411764707\n",
      "- Test set results:\n",
      "accuracy: 0.9985646325459318\n",
      "precision: 0.8391866913123844\n",
      "recall: 0.8376383763837638\n",
      "f1 score: 0.8384118190212372\n"
     ]
    }
   ],
   "source": [
    "# estimate model on trainset\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print(\"- Train set results:\")\n",
    "evaluate(y_train, y_pred_train)\n",
    "print(\"- Test set results:\")\n",
    "evaluate(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better, right? Of course that how much standardization affects your results will also vary depending on what ML algorithm you use. Some are more sensitive to it (e.g. kNN, SVM with RBF kernel and Gaussian Processes) than others (e.g. Decision Trees). And in some cases you may not even want to standardize your data at all (e.g. when you want to interpret the coefficients of a linear regression model)! But, in general, you should standardize your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's get back to analysing the results. You should have noticed something unusual in the results: the f1 score for the trainset is much better than the f1 score for the testset. Why do you think that happens? \n",
    "\n",
    "A possible reason is that the train distribution is different than the test distribution. This makes sense given the way we splitted the data: the train data is from between January and June, while the test data is from between August and December. But, in this case, could we have done something else? Unfortunatelly, not really... We need to respect the temporal order of the observations if we want our experimental setup to be realistic! Otherwise, it would be \"cheating\". In practice, when deployed, your ML algorithm will never have access to data from the future...\n",
    "\n",
    "But it is still interesting to consider this hypotheses and explore it further, so that we can know how much this train-test distribution mismatch is affecting the results of the model. To do so, we will make an experiment: we will ignore the temporal order of the observations, and perform a random split.\n",
    "\n",
    "Let us first recall the sizes of the previous train, validation and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train examples: 119117\n",
      "Num validation examples: 48005\n",
      "Num test examples: 121920\n"
     ]
    }
   ],
   "source": [
    "print(\"Num train examples:\", len(y_train))\n",
    "print(\"Num validation examples:\", len(y_val))\n",
    "print(\"Num test examples:\", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you now create a new data split - **give the variables a different name so that we don't overwrite the previous ones!** - with the same sizes as before, but where you first shuffle the Pandas Dataframe? You can use the function \"sklearn.utils.shuffle\" to shuffle a Pandas Dataframe. Please ensure that the different sets don't overlap and that they have the same sizes as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train examples: 119117\n",
      "Num validation examples: 48005\n",
      "Num test examples: 121920\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "shuffled_features = shuffle(labeled_features)\n",
    "\n",
    "# train/val/test split\n",
    "X_train_shuffled = shuffled_features[:119117][features_to_use]\n",
    "y_train_shuffled = shuffled_features[:119117][target_variables]\n",
    "X_val_shuffled = shuffled_features[119117:(119117+48005)][features_to_use]\n",
    "y_val_shuffled = shuffled_features[119117:(119117+48005)][target_variables]\n",
    "X_test_shuffled = shuffled_features[-121920:][features_to_use]\n",
    "y_test_shuffled = shuffled_features[-121920:][target_variables]\n",
    "\n",
    "# standardize data\n",
    "X_mean = X_train_shuffled.mean(axis=0)\n",
    "X_std = X_train_shuffled.std(axis=0)\n",
    "X_train_shuffled = (X_train_shuffled - X_mean) / X_std\n",
    "X_val_shuffled = (X_val_shuffled - X_mean) / X_std\n",
    "X_test_shuffled = (X_test_shuffled - X_mean) / X_std\n",
    "\n",
    "print(\"Num train examples:\", len(y_train_shuffled))\n",
    "print(\"Num validation examples:\", len(y_val_shuffled))\n",
    "print(\"Num test examples:\", len(y_test_shuffled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now fit the neural net again and evaluate it. Can you do it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stef/Documents/ba_intro/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set results:\n",
      "accuracy: 0.9989002409395804\n",
      "precision: 0.8707692307692307\n",
      "recall: 0.9233278955954323\n",
      "f1 score: 0.8962787015043547\n",
      "- Test set results:\n",
      "accuracy: 0.99873687664042\n",
      "precision: 0.8467374810318664\n",
      "recall: 0.9132569558101473\n",
      "f1 score: 0.878740157480315\n"
     ]
    }
   ],
   "source": [
    "# estimate model on trainset\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train_shuffled, y_train_shuffled)\n",
    "\n",
    "# make predictions\n",
    "y_pred_train = model.predict(X_train_shuffled)\n",
    "y_pred_test = model.predict(X_test_shuffled)\n",
    "\n",
    "print(\"- Train set results:\")\n",
    "evaluate(y_train_shuffled, y_pred_train)\n",
    "print(\"- Test set results:\")\n",
    "evaluate(y_test_shuffled, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the results now? Again, remember that they can vary a bit due to the randomness of the shuffling of the data. But you should now have obtained more similar results between train and test sets. Maybe the f1 score for the trainset is still a bit higher than testset, but they are all now close to each other. Therefore, this suggests that the differences between the different f1 scores that we observed before are indeed due to a train-test distribution mismatch! \n",
    "\n",
    "Ok, but why should we care? Well, this could inform us about various things:\n",
    "\n",
    "1) There is possibly a data sparsity/generalization issue, and that is a possible explanation for why shuffling the data before splitting increases the performance on the testset - since we shuffled the data, it is now more likely that we encounter in the trainset similar data points to the testset (e.g. 2 data points just 3h apart for the same failure situation). Hence, if we wish to further improve the quality of the model's predictions, collecting more data may be a good direction to invest!\n",
    "\n",
    "2) If we were to use this ML algorithm in practice, we now know that there possibly is a distribution drift in the data over time. Therefore, if we were to deploy this ML algorithm, it would probably be a good idea to re-train it often using the most recent data, and also using the largest amount of data possible (because of the previous point). \n",
    "\n",
    "Does that make sense to you? If not, do not hesitate to ask for a clarification! But the most important thing is that this encourages you to start thinking critically about the results that you get when doing ML :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Overfitting and underfitting\n",
    "\n",
    "We will now study the issues of overfitting and underfitting. Actually, we can see the train/test distribution mismatch also as an overfitting problem. The model is \"too focused\" on fitting the data points in the trainset, and it fails to generalize well to slightly different data points (like the ones in the test set). \n",
    "\n",
    "Let us recap the logistic model from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stef/Documents/ba_intro/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set results:\n",
      "accuracy: 0.9989422164762376\n",
      "precision: 0.8764204545454546\n",
      "recall: 0.9405487804878049\n",
      "f1 score: 0.9073529411764707\n",
      "- Test set results:\n",
      "accuracy: 0.9985646325459318\n",
      "precision: 0.8391866913123844\n",
      "recall: 0.8376383763837638\n",
      "f1 score: 0.8384118190212372\n"
     ]
    }
   ],
   "source": [
    "# estimate model on trainset\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print(\"- Train set results:\")\n",
    "evaluate(y_train, y_pred_train)\n",
    "print(\"- Test set results:\")\n",
    "evaluate(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now consider a very popular technique for combating overfitting in logistic regression models (and also linear regression, neural networks, etc.): **regularization**.\n",
    "\n",
    "For example, $\\ell_2$-regulrization works by extending the objective function that is minimized during the training of a logistic regression model with the following term: \n",
    "\n",
    "$\\dots + \\lambda \\sum_{j} w_j^2$\n",
    "\n",
    "You can see this extra term as penalty for the weights $\\{w_j\\}$ getting too large. Large weights will cause large fluctuations of the output with small changes in the input - violates smooth assumption of the output, and therefore leads to overfitting.\n",
    "\n",
    "In the logistic regression implementation of sklearn, this term is already considered by default. We can control the amount of regularization by changing the value of the parameter $\\lambda$ - called \"C\" in the sklearn interface (note that in sklearn it is parametrized as $C=1/\\lambda$). Example: \n",
    "\n",
    "> model = LogisticRegression(C=1)\n",
    "\n",
    "Therefore, you can see the \"C\" as controlling the smoothness of the decision boundary learned by the logistic regression classifier:\n",
    "\n",
    "<div>\n",
    "    <br/>\n",
    "    <img src=\"https://miro.medium.com/max/1648/1*JZbxrdzabrT33Yl-LrmShw.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Can you give it a try and see how it affects the results? (the default value in sklearn is \"C=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stef/Documents/ba_intro/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set results:\n",
      "accuracy: 0.998875055617586\n",
      "precision: 0.8717948717948718\n",
      "recall: 0.9329268292682927\n",
      "f1 score: 0.9013254786450663\n",
      "- Test set results:\n",
      "accuracy: 0.9986384514435696\n",
      "precision: 0.8443223443223443\n",
      "recall: 0.8505535055350554\n",
      "f1 score: 0.8474264705882353\n"
     ]
    }
   ],
   "source": [
    "# estimate model on trainset\n",
    "model = LogisticRegression(C=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print(\"- Train set results:\")\n",
    "evaluate(y_train, y_pred_train)\n",
    "print(\"- Test set results:\")\n",
    "evaluate(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you noticed the effect of the value of \"C\"? Higher values of C (e.g. C > 1) lead to more overfitting. But a lower value of C can be use to prevent overfitting. Try setting C=0.1. What happened? You should have noticed that the trainset f1 score went down, but the f1 score for the testset actually increased! So, by setting C=0.1, we increased the strenght of the penalty $\\lambda = 1/C$, and by doing so, we managed to reduce overfitting and that reduction actually improved the generalization ability of the model!\n",
    "\n",
    "**One important note:** Like any other model hyper-parameter, the \"optimal\" value of C should be tuned using the validation set! Not by relying on the results on the testset.\n",
    "\n",
    "But, in this notebook, we are just interesting in comprehending the effect of regularization. Try setting C=0.01. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stef/Documents/ba_intro/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set results:\n",
      "accuracy: 0.9982706078897219\n",
      "precision: 0.8787878787878788\n",
      "recall: 0.7957317073170732\n",
      "f1 score: 0.8352\n",
      "- Test set results:\n",
      "accuracy: 0.9982283464566929\n",
      "precision: 0.8638392857142857\n",
      "recall: 0.7140221402214022\n",
      "f1 score: 0.7818181818181816\n"
     ]
    }
   ],
   "source": [
    "# estimate model on trainset\n",
    "model = LogisticRegression(C=0.01, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print(\"- Train set results:\")\n",
    "evaluate(y_train, y_pred_train)\n",
    "print(\"- Test set results:\")\n",
    "evaluate(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For C=0.01, the model underfits! The decision boundary learned is so \"smooth\" that it does not even has enough flexibility to perform well on the trainset. Therefore, the trainset f1 went down from 0.90 to 0.79. Moreover, as a consequence of this decresed flexibility of the model, the testset f1 also went down.\n",
    "\n",
    "As you hopefully were able to notice from this simple experiment, controlling overfitting can be extremely difficult. That is why it is considered one of the main problems in ML (if not THE main problem...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: L1 vs L2 regularization\n",
    "\n",
    "But there is another popular type of regularization: $\\ell_1$-regularization (also known as the \"LASSO\" penalty). It works by extending the objective function that is minimized during the training of a logistic regression model with the following term: \n",
    "\n",
    "$\\dots + \\lambda \\sum_{j} |w_j|$\n",
    "\n",
    "Compare this penalty with the term used by $\\ell_2$-regularization. What is the effect of taking the absolute value instead of the square? Here is a visual comparison of the penalty value (y-axis) for different values of $w$ (x-axis). The blue line shows the penalty of $\\ell_2$-regularization, while the red line corresponds to $\\ell_1$-regularization.\n",
    "\n",
    "<div>\n",
    "    <br/>\n",
    "    <img src=\"https://miro.medium.com/max/1056/1*AgA43f_6wcNKZX4p-FmL8w.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Notice how with $\\ell_1$-regularization (red line) the value of penalty increases dramatically when you move just a little a bit away from $w=0$ in the x-axis. As a consequence, $\\ell_1$-regularization is sparsity inducing, i.e. it encourages the weight of less important features to go towards zero. \n",
    "\n",
    "Let's make a comparison between $\\ell_1$ and $\\ell_2$-regularization in our logistic regression classifier from before. We start by running it again with \"C=0.1\", and having a look at the learned coefficients (or weights, or parameters... whatever you want to call them :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stef/Documents/ba_intro/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set results:\n",
      "accuracy: 0.998875055617586\n",
      "precision: 0.8717948717948718\n",
      "recall: 0.9329268292682927\n",
      "f1 score: 0.9013254786450663\n",
      "- Test set results:\n",
      "accuracy: 0.9986384514435696\n",
      "precision: 0.8443223443223443\n",
      "recall: 0.8505535055350554\n",
      "f1 score: 0.8474264705882353\n"
     ]
    }
   ],
   "source": [
    "# estimate model on trainset\n",
    "model = LogisticRegression(C=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print(\"- Train set results:\")\n",
    "evaluate(y_train, y_pred_train)\n",
    "print(\"- Test set results:\")\n",
    "evaluate(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.29607115,  0.00396775, -0.0035466 ,  0.01392055, -0.02118806,\n",
       "         0.06778626, -0.00157222,  0.01501246,  0.97056163,  0.05785912,\n",
       "         0.05400005, -0.21277415,  0.01155817, -0.05058435,  0.07034745,\n",
       "         0.0502811 ,  0.82904766, -0.01616311,  0.09226561,  0.08745954,\n",
       "         0.02874838,  0.2502439 ,  0.04035685, -0.00621036,  0.09723908,\n",
       "         0.08459318, -0.08820161,  0.01895567, -0.01484123,  0.02235715]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit the logistic regression classifier again, but let's switch the regularization type to $\\ell_1$. This can be done adding the keyword parameter \"penalty='l1'\" to the \"LogisticRegression()\" constructor call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stef/Documents/ba_intro/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set results:\n",
      "accuracy: 0.998891845832249\n",
      "precision: 0.8669467787114846\n",
      "recall: 0.9435975609756098\n",
      "f1 score: 0.9036496350364963\n",
      "- Test set results:\n",
      "accuracy: 0.9986712598425197\n",
      "precision: 0.841726618705036\n",
      "recall: 0.8634686346863468\n",
      "f1 score: 0.8524590163934426\n"
     ]
    }
   ],
   "source": [
    "# estimate model on trainset\n",
    "model = LogisticRegression(penalty='l1', C=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print(\"- Train set results:\")\n",
    "evaluate(y_train, y_pred_train)\n",
    "print(\"- Test set results:\")\n",
    "evaluate(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41425212,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.05856836,  0.        ,  0.        ,  1.40553646,  0.03550125,\n",
       "         0.04014496, -0.26327514,  0.        , -0.09588407,  0.07574996,\n",
       "         0.07125126,  1.21083649,  0.        ,  0.03663398,  0.08374954,\n",
       "         0.        ,  0.38993053,  0.        ,  0.        ,  0.11673502,\n",
       "         0.1140358 , -0.13493871,  0.        , -0.04004162,  0.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of those coefficients? A lot of them are zero! What do you think that means in a linear model like logistic regression? If the coeffient is zero, then that feature takes absolutely no role in determining the class according to that model - i.e. the feature is deemed irrelevant. Interestingly, in this case, it seems that doing so leads to better generalization to the testset (better f1 score in the testset). So, rather than helping the classifier, maybe those features were actually hurting the performance of the model... Removing irrelevant features helps dealing with the curse of dimensionality! \n",
    "\n",
    "Write a piece of code that looks up feature names in the array \"features_to_use\" declared above that correspond to those zeros - i.e. the names of the features deemed irrelevant by the logistic regression model with $\\ell_1$-regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature rotatemean_3h is irrelevant\n",
      "Feature pressuremean_3h is irrelevant\n",
      "Feature vibrationmean_3h is irrelevant\n",
      "Feature voltsd_3h is irrelevant\n",
      "Feature pressuresd_3h is irrelevant\n",
      "Feature vibrationsd_3h is irrelevant\n",
      "Feature voltsd_24h is irrelevant\n",
      "Feature error2count is irrelevant\n",
      "Feature error5count is irrelevant\n",
      "Feature comp2 is irrelevant\n",
      "Feature comp3 is irrelevant\n",
      "Feature model_model3 is irrelevant\n",
      "Feature age is irrelevant\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(features_to_use)):\n",
    "    if model.coef_[0][i] == 0:\n",
    "        print(\"Feature %s is irrelevant\" % (features_to_use[i],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Regularization in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this last part we will consider overfitting and regularization techniques in neural networks. \n",
    "\n",
    "Since we will be training and testing quite a few different models, let us define two functions:\n",
    "\n",
    "```\n",
    "model, history = fit_nnet(X_train, y_train, X_val, y_val, num_epochs=15, batch_size=2048) # fits a neural network to the data\n",
    "```\n",
    "\n",
    "```\n",
    "eval_nnet(model, X_new, y_true) # evaluates the performance of the provided \"model\" in the provided data (\"X_new\", \"y_true\")\n",
    "```\n",
    "\n",
    "Note that the \"fit_nnet\" method uses a relatively straighforward neural network architecture. This is not necessarily the best architecture for this particular problem, but that is not the purpose of this notebook anyway. Our focus will be on other concepts.\n",
    "\n",
    "Make sure the following code makes sense. You probably implemented something similar in the last lecture..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fit nnet\n",
    "def fit_nnet(X_train, y_train, X_val, y_val, num_epochs=15, batch_size=2048):\n",
    "    # define the keras model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=30, activation='relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compile the keras model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # fit the keras model on the dataset\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val,y_val),\n",
    "                        epochs=num_epochs, batch_size=batch_size, verbose=2)\n",
    "    return model, history\n",
    "\n",
    "# function to evaluate nnet on some data\n",
    "def eval_nnet(model, X_new, y_true):\n",
    "    # evaluate the keras model\n",
    "    y_pred = model.predict(X_new)\n",
    "    y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    # evaluate predictions\n",
    "    evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the functions above to fit a neural net to the standardized predictive maintenance data from before and compute the error statistics on the train, validation and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119117 samples, validate on 48005 samples\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.1130 - accuracy: 0.9738 - val_loss: 0.0167 - val_accuracy: 0.9945\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0192 - accuracy: 0.9946 - val_loss: 0.0067 - val_accuracy: 0.9945\n",
      "Epoch 3/10\n",
      " - 3s - loss: 0.0107 - accuracy: 0.9950 - val_loss: 0.0051 - val_accuracy: 0.9979\n",
      "Epoch 4/10\n",
      " - 3s - loss: 0.0077 - accuracy: 0.9958 - val_loss: 0.0043 - val_accuracy: 0.9989\n",
      "Epoch 5/10\n",
      " - 3s - loss: 0.0064 - accuracy: 0.9967 - val_loss: 0.0036 - val_accuracy: 0.9994\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0051 - accuracy: 0.9977 - val_loss: 0.0030 - val_accuracy: 0.9995\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0046 - accuracy: 0.9981 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0043 - accuracy: 0.9982 - val_loss: 0.0023 - val_accuracy: 0.9995\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0037 - accuracy: 0.9986 - val_loss: 0.0020 - val_accuracy: 0.9996\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.0032 - accuracy: 0.9988 - val_loss: 0.0019 - val_accuracy: 0.9995\n"
     ]
    }
   ],
   "source": [
    "fitted_model, history = fit_nnet(X_train, y_train, X_val, y_val, num_epochs=10, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set results:\n",
      "accuracy: 0.9991940696961811\n",
      "precision: 0.8723404255319149\n",
      "recall: 1.0\n",
      "f1 score: 0.9318181818181819\n",
      "- Test set results:\n",
      "accuracy: 0.9990403543307087\n",
      "precision: 0.8595600676818951\n",
      "recall: 0.9372693726937269\n",
      "f1 score: 0.8967343336275375\n"
     ]
    }
   ],
   "source": [
    "print(\"- Train set results:\")\n",
    "eval_nnet(fitted_model, X_train, y_train)\n",
    "print(\"- Test set results:\")\n",
    "eval_nnet(fitted_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are trained using stochastic gradient methods (in this case, it used an optimizer called \"adam\") on a non-convex objective function (or \"loss function\"). Therefore, convergence to a global optimum is not guaranteed. \n",
    "<div>\n",
    "    <br/>\n",
    "    <img src=\"https://qph.fs.quoracdn.net/main-qimg-f848fbbcbf279aadeacb7bd9850d5ed1\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "When working with neural networks, it is always a good practice to plot the evolution of the train and validation losses (\"losses\" = \"objective function being optimized\") over time during training. This can be done using the \"history\" variable returned by the \"model.fit(...)\" function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.legend([\"train loss\", \"val loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the same plot for the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-2718187f2e91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'acc'"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history[\"acc\"])\n",
    "plt.plot(history.history[\"val_acc\"])\n",
    "plt.legend([\"train accuracy\", \"val accuracy\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do those plots look to you? Do you think there is any overfitting occurring? Has training converged? Or maybe we could get a better results by training for more iterations (epochs)?\n",
    "\n",
    "Give it a try. Train the neural network using 50 epochs and plot the loss values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fitted_model, history = fit_nnet(X_train, y_train, X_val, y_val, num_epochs=50, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- Train set results:\")\n",
    "eval_nnet(fitted_model, X_train, y_train)\n",
    "print(\"- Test set results:\")\n",
    "eval_nnet(fitted_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.legend([\"train loss\", \"val loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is starting to overfit, isn't it? The loss on the trainset keeps decreasing, but the loss on the validation set starts to increase. We probably should have stopped around iterations 10-15...\n",
    "\n",
    "Notice that this overfit led to a quite significant decrease in f1 score in the testset! :-("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "A very popular regularization technique in neural nets is Dropout. In the \"fit_nnet(...)\" function that we defined above, we used Dropout. What happens if we now re-define the \"fit_nnet\" function but without the Dropout? Can you try? Let it train also for 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fit nnet\n",
    "def fit_nnet(X_train, y_train, X_val, y_val, num_epochs=15, batch_size=2048):\n",
    "    # define the keras model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=30, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compile the keras model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # fit the keras model on the dataset\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val,y_val),\n",
    "                        epochs=num_epochs, batch_size=batch_size, verbose=2)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fitted_model, history = fit_nnet(X_train, y_train, X_val, y_val, num_epochs=50, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- Train set results:\")\n",
    "eval_nnet(fitted_model, X_train, y_train)\n",
    "print(\"- Test set results:\")\n",
    "eval_nnet(fitted_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.legend([\"train loss\", \"val loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is way worse now! We can conclude that Dropout was indeed helping quite a lot at preventing overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization in Neural Networks\n",
    "\n",
    "Like with logistic regression, we can add $\\ell_1$ or $\\ell_2$ penalty terms to the neural net objective function. In Keras, this is done simply by adding \"kernel_regularizer=regularizers.l2(...)\" to the layer definition:\n",
    "\n",
    "```\n",
    "from keras import regularizers\n",
    "model.add(Dense(..., kernel_regularizer=regularizers.l2(0.001)))\n",
    "```\n",
    "\n",
    "The parameter of \"regularizers.l2(...)\" controls the strenght of the regularization. \n",
    "\n",
    "Can you try updating the \"fit_nnet(...)\" function from above to use $\\ell_2$-regularization in the neural network? Try for example using \"regularizers.l2(0.001)\". Don't use Dropout. Train the neural net for 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "# function to fit nnet\n",
    "def fit_nnet(X_train, y_train, X_val, y_val, num_epochs=15, batch_size=2048):\n",
    "    # define the keras model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=30, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dense(100, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dense(30, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.001)))\n",
    "\n",
    "    # compile the keras model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # fit the keras model on the dataset\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val,y_val),\n",
    "                        epochs=num_epochs, batch_size=batch_size, verbose=2)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fitted_model, history = fit_nnet(X_train, y_train, X_val, y_val, num_epochs=50, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- Train set results:\")\n",
    "eval_nnet(fitted_model, X_train, y_train)\n",
    "print(\"- Test set results:\")\n",
    "eval_nnet(fitted_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.legend([\"train loss\", \"val loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effective, right? Overfitting seems to be gone and the train and testset f1 scores are now again close to each other (around 0.9). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Let's then recap what we learned from this notebook:\n",
    "\n",
    "- Be careful with having different train and test set distributions - we saw a possible way to identify this issue and its potential impact on the results\n",
    "\n",
    "- Overfitting can significatively impact your results - it is one the major issues in ML!\n",
    "\n",
    "- We saw how regularization can be used to prevent overfitting\n",
    "\n",
    "- We learned the differences between L1 with L2 regularization\n",
    "\n",
    "- Dropout can be a very effect way of preventing overfitting\n",
    "\n",
    "- L1 and L2-regularization can also be used in neural networks\n",
    "\n",
    "We hope that you enjoyed this notebook and that you managed to get some new (and very practical!) insights from it :-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
