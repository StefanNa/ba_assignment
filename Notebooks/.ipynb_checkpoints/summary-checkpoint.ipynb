{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, precision_score\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#matplotlib style options\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#type category\n",
    "de['model'] = df['model'].astype('category')\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "#merge\n",
    "features = df1.merge(df2, on=['datetime', 'machineID'], how='left')\n",
    "\n",
    "#filna\n",
    "df = df.fillna(method='bfill', limit=7) # fill backward up to limit entries \n",
    "df = df.fillna('none')\n",
    "\n",
    "#category to binary:\n",
    "df['comp1_fail'] = (df['failure'] == 'comp1').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate\n",
    "def evaluate(y_true, y_pred, print_cm=False):\n",
    "    # calculate and display confusion matrix\n",
    "    labels = np.unique(y_true)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    if print_cm:\n",
    "        print('Confusion matrix\\n- x-axis is true labels (none, comp1, etc.)\\n- y-axis is predicted labels')\n",
    "        print(cm)\n",
    "\n",
    "    # calculate precision, recall, and F1 score\n",
    "    accuracy = float(np.trace(cm)) / np.sum(cm)\n",
    "    precision = precision_score(y_true, y_pred, average=None, labels=labels)[1]\n",
    "    recall = recall_score(y_true, y_pred, average=None, labels=labels)[1]\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print(\"accuracy:\", accuracy)\n",
    "    print(\"precision:\", precision)\n",
    "    print(\"recall:\", recall)\n",
    "    print(\"f1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle data\n",
    "from sklearn.utils import shuffle\n",
    "shuffled_features = shuffle(df)\n",
    "\n",
    "# standardize data\n",
    "X_mean = X_train_shuffled.mean(axis=0)\n",
    "X_std = X_train_shuffled.std(axis=0)\n",
    "X_train_shuffled = (X_train_shuffled - X_mean) / X_std\n",
    "X_val_shuffled = (X_val_shuffled - X_mean) / X_std\n",
    "X_test_shuffled = (X_test_shuffled - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#or with l2 regularization\n",
    "model = LogisticRegression(C=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#or l1 regularization # shows irrelevant features\n",
    "model = LogisticRegression(penalty='l1', C=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "# find irrelevant features\n",
    "for i in range(len(features_to_use)):\n",
    "    if model.coef_[0][i] == 0:\n",
    "        print(\"Feature %s is irrelevant\" % (features_to_use[i],))\n",
    "        \n",
    "####\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print(\"- Train set results:\")\n",
    "evaluate(y_train, y_pred_train)\n",
    "print(\"- Test set results:\")\n",
    "evaluate(y_test, y_pred_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# function to fit nnet\n",
    "def fit_nnet(X_train, y_train, X_val, y_val, num_epochs=15, batch_size=2048):\n",
    "    # define the keras model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=30, activation='relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compile the keras model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # fit the keras model on the dataset\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val,y_val),\n",
    "                        epochs=num_epochs, batch_size=batch_size, verbose=2)\n",
    "    return model, history\n",
    "\n",
    "# function to evaluate nnet on some data\n",
    "def eval_nnet(model, X_new, y_true):\n",
    "    # evaluate the keras model\n",
    "    y_pred = model.predict(X_new)\n",
    "    y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    # evaluate predictions\n",
    "    evaluate(y_true, y_pred)\n",
    "    \n",
    "\n",
    "fitted_model, history = fit_nnet(X_train, y_train, X_val, y_val, num_epochs=10, batch_size=2048)\n",
    "\n",
    "print(\"- Train set results:\")\n",
    "eval_nnet(fitted_model, X_train, y_train)\n",
    "print(\"- Test set results:\")\n",
    "eval_nnet(fitted_model, X_test, y_test)\n",
    "\n",
    "#plot gistory\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.legend([\"train loss\", \"val loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-af8ab55e2cee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Add columns with year, month, and weekday name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Year'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Month'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Weekday Name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweekday_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Add columns with year, month, and weekday name\n",
    "df['Year'] = df.index.year\n",
    "df['Month'] = df.index.month\n",
    "df['Weekday Name'] = df.index.weekday_name\n",
    "\n",
    "#select specific day\n",
    "df.loc['2017-08-10']\n",
    "df.loc['2014-01-20':'2014-01-22']\n",
    "\n",
    "#drop na\n",
    "df.dropna()\n",
    "\n",
    "# seaborn styling for our plots, and letâ€™s adjust the \n",
    "# default figure size to an appropriate shape for time series plots.\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(14, 5)})\n",
    "\n",
    "#plot selected features\n",
    "cols_plot = ['Consumption', 'Solar', 'Wind']\n",
    "axes = df[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(14, 11), subplots=True)\n",
    "for ax in axes:\n",
    "    ax.set_ylabel('Daily Totals (GWh)')\n",
    "\n",
    "#loc certain time and plot 1 feature\n",
    "ax = df.loc['2017-01':'2017-02', 'Consumption'].plot()\n",
    "ax.set_ylabel('Daily Consumption (GWh)');\n",
    "\n",
    "#weekends\n",
    "weekends=df.loc['2017-01':'2017-02', 'Consumption'].index.weekday>=5\n",
    "colors=['blue' if x else 'red' for x in weekends]\n",
    "\n",
    "#formatted plot for weeks\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(opsd_daily.loc['2017-01':'2017-02', 'Consumption'])\n",
    "ax.scatter(opsd_daily.loc['2017-01':'2017-02'].index, opsd_daily.loc['2017-01':'2017-02', 'Consumption'], marker='o', linestyle='-', c=colors)\n",
    "ax.set_ylabel('Daily Consumption (GWh)')\n",
    "ax.set_title('Jan-Feb 2017 Electricity Consumption')\n",
    "\n",
    "# To better visualize the weekly seasonality in electricity consumption \n",
    "# we add vertical gridlines on a weekly time scale\n",
    "\n",
    "# Set x-axis major ticks to weekly interval, on Mondays\n",
    "ax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=mdates.MONDAY))\n",
    "# Format x-tick labels as 3-letter month name and day number\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'));\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOXPLOTS\n",
    "fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)\n",
    "for name, ax in zip(['Consumption', 'Solar', 'Wind'], axes):\n",
    "    sns.boxplot(data=df, x='Month', y=name, ax=ax)#need to add month collumn before\n",
    "    ax.set_ylabel('GWh')\n",
    "    ax.set_title(name)\n",
    "    # Remove the automatic x-axis label from all but the bottom subplot\n",
    "    if ax != axes[-1]:\n",
    "        ax.set_xlabel('')\n",
    "        \n",
    "#for weekdays\n",
    "sns.boxplot(data=df, x='Weekday Name', y='Consumption');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resampling\n",
    "\n",
    "# Specify the data columns we want to include (i.e. exclude Year, Month, Weekday Name)\n",
    "data_columns = ['Consumption', 'Wind', 'Solar', 'Wind+Solar', 'gap', 'diff']\n",
    "# Resample to weekly frequency, aggregating with mean\n",
    "opsd_weekly_mean = df[data_columns].resample('W').mean()\n",
    "opsd_weekly_mean[:3]\n",
    "\n",
    "#rolling windows\n",
    "opsd_7d = df[data_columns].rolling(7, center=True).mean()\n",
    "opsd_365d = df[data_columns].rolling(window=365, center=True, min_periods=360).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autocorrelation\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "plot_acf(opsd_daily['Consumption'])\n",
    "plot_pacf(opsd_daily['Consumption']);\n",
    "#looks at the correlation every lag of X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings                                  # `do not disturbe` mode\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "from sklearn.model_selection import TimeSeriesSplit # you have everything done for you\n",
    "\n",
    "\n",
    "#evaluate results\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "#plot original and predicted\n",
    "def plotprediction(series, pred_series, labels=[\"original\", \"predicted\"], x_axis=None, plot_intervals=False, scale=1.96, plot_anomalies=False, title=\"prediction\"):\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.title(title)\n",
    "    if x_axis is None:\n",
    "        x_axis=series.index\n",
    "    \n",
    "    plt.plot(x_axis, pred_series, \"g\", label=labels[1])\n",
    "    plt.plot(x_axis, series, label=labels[0])\n",
    "    \n",
    "\n",
    "    # Plot confidence intervals for smoothed values\n",
    "\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "def timeseries_train_test_split_indexes(ts, test_size):\n",
    "    \"\"\"\n",
    "        Gets the two vectors with indexes for the train test split (first vector is train observations, \n",
    "        second vector is test observations).\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the index after which test set starts\n",
    "    split_time = int(len(ts)*(1-test_size))\n",
    "    \n",
    "    test_index=ts.iloc[split_time:].index[0]\n",
    "    \n",
    "    return ts.index<test_index, ts.index>=test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train test split\n",
    "ads_train_ix, ads_test_ix=timeseries_train_test_split_indexes(df, 0.3)\n",
    "\n",
    "#look at basic lag of 1\n",
    "df['copy_pred']=[0]+df['Ads'].to_list()[:-1]\n",
    "#plot\n",
    "plotprediction(df[ads_test_ix]['Ads'], df[ads_test_ix]['copy_pred'], labels=['observed', 'copy based prediction'], title=\"Naive method 1\")\n",
    "\n",
    "#stats\n",
    "print_stats(ads[ads_test_ix]['Ads'], ads[ads_test_ix]['copy_pred'], \"copy method\")\n",
    "\n",
    "#apply rolling and print\n",
    "plotprediction(df['GEMS_GEMS_SPENT'], df['GEMS_GEMS_SPENT'].rolling(4).mean(), x_axis=currency.index,labels=['observed', 'moving average smoothing (p=24)'], title=\"Smoothing\")\n",
    "\n",
    "\n",
    "#look at autocorrelation\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "plot_acf(ads_data['Ads'])\n",
    "plot_pacf(ads_data['Ads']);\n",
    "####look at PACF for AR (count ones above interval)\n",
    "####look at ACF for MA (countones clearly above interval)\n",
    "\n",
    "\n",
    "#basic linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "y = ads_data.Ads\n",
    "X = ads_data.drop(['Ads'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[ads_train_ix], X[ads_test_ix], y[ads_train_ix], y[ads_test_ix]\n",
    "\n",
    "X_train=X_train[LAGS:]  #because the first LAGS observations will have NaNs\n",
    "y_train=y_train[LAGS:]\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred=lr.predict(X_test)\n",
    "\n",
    "plotprediction(y_test, y_pred)\n",
    "\n",
    "\n",
    "#check result\n",
    "print_stats(y_test, y_pred, \"AR (3)\")\n",
    "\n",
    "\n",
    "# MA example\n",
    "from statsmodels.tsa.arima_model import ARMA, ARIMA\n",
    "\n",
    "\n",
    "##ARIMA\n",
    "# run adf to se if integration is necessary\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "result = adfuller(ads['Ads'])\n",
    "print('ADF Statistic: %f' % result[0]) #the more negative the higher the chance of stationary (1% should be higher)\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "   print('\\t%s: %.3f' % (key, value))\n",
    "# https://machinelearningmastery.com/time-series-data-stationary-python/\n",
    "\n",
    "preds=[]\n",
    "for l in ads[ads_test_ix]['Ads'].index:\n",
    "    data=ads['Ads'][ads.index<l]\n",
    "    model=ARIMA(data, order=(3, 0, 2)) #Notice that now we're using the ARIMA object\n",
    "    arima=model.fit()\n",
    "    \n",
    "    preds.append(arima.forecast()[0])\n",
    "    \n",
    "y_pred=np.array(preds).T[0]\n",
    "print(arima.summary())\n",
    "\n",
    "#plot\n",
    "y_test=ads[ads_test_ix]['Ads']\n",
    "plotprediction(y_test, y_pred)\n",
    "\n",
    "print_stats(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
